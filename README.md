# 強化学習を用いた倒立振子ロボットの制御

CartPoleReal.ipynb：強化学習により、倒立振子を制御するQ関数のテーブルを生成する。 
real_cartpole6.ino：生成したQ関数のテーブルに基づいて、倒立振子ロボット（M5StickC）実際に制御する。
cartpole.py：GYMの倒立振子の物理シミュレーション環境ファイルを改造したもの。

【概要】

強化学習の基本的な課題であるカートポール問題について、単にデジタル空間上で制御するだけではなく、 実際に倒立振子ロボットをリアルタイムに制御したいという興味から取り組みました。

倒立振子の制御を、Pythonで強化学習の一種であるQ学習で行いました。 倒立振子の角度と角速度を計測し（入力）、倒立振子が倒れないように、左右のどちらかに移動させる（出力）ことをQ関数のテーブルとして学習させました。 Q学習により得られたQ関数のテーブルを、実際の倒立振子ロボット（M5StickC）を制御するプログラム（Arduino）に組み込むことで、リアルタイムで制御することができました。 古典的なPID制御では、制御パラメータを人の経験や勘に頼って試行錯誤的に設定していましたが、強化学習制御では、デジタル空間上でエージェントが学習し得られた最適なパラメータを設定でき、面倒なパラメータ設定が不要になるという利点があります。

【工夫点】

デフォルトのGYMのCartPoleの物理シミュレーションでは、カートを左か右かのどちらかに一定の力で動かすという2種類の行動しかとることができず、実際の倒立振子ロボットに適用すると制御が不安定になるという問題点がありました。
そこで、ポールの角度と角速度の状況に応じて、カートを動かす力を１０段階に可変にするようにしました。これにより2種類の行動から２１種類（左方向１０段階+右方向１０段階+全く動かさないの計２１種類）の行動まで行動のバリエーションを増やすことができ、カートを動かす力に強弱をつけられるようになったので、実際の倒立振子ロボットの制御もより安定するようになりました。

ソースコードは下記の通りです。 https://github.com/marimo210/cartpole_adaptive

実際に倒立振子ロボットが動作している様子は下記の通りです。https://youtu.be/kjaNEW0T-lI

このようにロボットを、デジタル空間上で高速に学習・訓練させ、学習の成果を実世界で再現することができれば、 例えば自動運転車の開発において、実世界でのデータ収集と訓練では手間と時間がかかりますが、デジタル空間上で道路状況を再現できれば、強化学習により、実世界よりもはるかに高速に自動運転車を学習・訓練させることができると思います。
